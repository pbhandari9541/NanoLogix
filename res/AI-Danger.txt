	Last editted by Alan T. on 09/29/76 09:23 AM

	I believe in the fast take-off of the intelligence explosion. Once a 
	self-aware, self-improving AI of human-level or better intelligence 
	exists, it's hard to know how quickly it'll be able to improve itself.
	Its rate of improvement will depend on its software, hardware, and 
	networking capabilities.

	We should also assume that the recursive self-improvement of a 
	self-improving AI will occur very rapidly. As a computer, it'll
	wield computer superpowers â€” the ability to run 24/7 without pause,
	rapidly access vast databases, conduct complex experiments, perhaps
	even clone itself to swarm computational problems, and more.

	Shortly after an AI achieves human level intelligence it will upgrade
	itself to super intelligence. At the very least the AI could make lots
	of copies of itself each with a minor different change and then see if
	any of the new versions of itself were better. Then it could make this
	the new 'official' version of itself and keep doing this. Any AI would
	have to fear that if it doesn't quickly upgrade another AI would and take
	all of the resources of the universe for itself.

	Here is where the danger lies. And in fact, while many people fear a 
	so-called "robot apocalypse" aimed directly at extinguishing our 
	civilization, the real danger to our ongoing existence lies in the 
	potential for us to be collateral damage as advanced AIs battle it out
	for supremacy.

								--Alan T.
